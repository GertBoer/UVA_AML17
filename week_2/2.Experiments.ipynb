{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 1\n",
    "## Experiments\n",
    "In this part of the assignment you will use the blocks of code which you wrote in the first part of the assignment for the image classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "* [Digits Classification Task](#1.-Digits Classification Task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits Classification Task\n",
    "In this task you will implement a neural network for classification. You can use the blocks of code which you implemented in the first part of this assignment for completing this task.\n",
    "\n",
    "We will use **digits** dataset for this task. This dataset consists of 1797 8x8 images. Further information about the dataset can be found [here](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load this dataset from scikit-learn using the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADlCAYAAABzoh0gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nABAMwQ0AwRDcABAMwQ0AwRDcABDM/wPpgfDdmNP0UgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18caea3f7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "# We load the dataset\n",
    "digits = sklearn.datasets.load_digits()\n",
    "\n",
    "# Here we load up the images and labels and print some examples\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: {}'.format(label), y=1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will divide the images and labels data into two parts i.e. training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_objects = digits.images.shape[0]\n",
    "train_test_split = 0.7\n",
    "train_size = int(n_objects * train_test_split)\n",
    "indices = np.arange(n_objects)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "train_images, train_targets = digits.images[train_indices], digits.target[train_indices]\n",
    "test_images, test_targets = digits.images[test_indices], digits.target[test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images in the dataset are $8 \\times 8$ and each pixel in the image is eithe 0 or 1. Before giving the images as input to the neural network we will reshape them to 1 by 64 times long 1 dimensional vector as shown in the figure below.\n",
    "![in](./images/image_pixel_input.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((-1, 64))\n",
    "test_images = test_images.reshape((-1, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic units of the neural network are perceptrons. A perceptron consists of a cell with atleast two inputs. Cell takes the inputs multiplied with weights and gives an output after computing the values. The basic diagram of a cell is shown below.\n",
    "![neuron](./images/neuron.png)\n",
    "For the image dataset which we will be using in this task the perceptron will have 64 inputs for $8 \\times 8$ input and 64 weights.\n",
    "![N_weights](./images/weights.png)\n",
    "As the digits dataset consists of 10 classes (0 to 9) so, in order to classifiy the images we will need 10 neurons for the prediction of the target class. It can be seen from the image each neuron will give an output and the output from the neuron with the highest value will be selected and that will be the predicted output.\n",
    "![NN](./images/design.png)\n",
    "Now, in order to perform classification task for images you will use the functions which you implemented in the first task of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_forward(x_input, W, b):\n",
    "    \"\"\"Matrix multiplication operation\n",
    "    # Arguements\n",
    "        x_input:def dense_grad_input(x_input, grad_output, W, b):\n",
    "    Matrix multiplication gradient\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    \"\"\"\n",
    "    return grad_input np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_grad_input(x_input, grad_output, W, b):\n",
    "    \"\"\"Matrix multiplication gradient\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"W gradient computation\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_W\n",
    "\n",
    "def dense_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"b gradient computation\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_phase = True\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return grad_output\n",
    "    \n",
    "    def get_params(self):\n",
    "        return []\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Dense, self).__init__()\n",
    "        #Randomly initializing the weights from normal distribution\n",
    "        self.W = np.random.normal(size=(n_input, n_output))\n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        #initializing the bias with zero\n",
    "        self.b = np.zeros(n_output)\n",
    "        self.grad_b = np.zeros_like(self.b)\n",
    "      \n",
    "    def forward(self, x_input):\n",
    "        self.output = dense_forward(x_input, self.W, self.b)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        # get gradients of weights\n",
    "        self.grad_W = dense_grad_W(x_input, grad_output, self.W, self.b)\n",
    "        self.grad_b = dense_grad_b(x_input, grad_output, self.W, self.b)\n",
    "        # propagate the gradient backwards\n",
    "        return dense_grad_input(x_input, grad_output, self.W, self.b)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have implemented the advanced section of the first part of assignment then by completing the next blocks for *drop out* implementation you can check the dropout for image classification task. But as the implementation of *drop out* was optional so, if you didn't implement it you can skip this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_generate_mask(shape, drop_rate):\n",
    "    \"\"\"Generate mask \n",
    "    # Arguements\n",
    "        shape: shape of the input array \n",
    "            tuple \n",
    "        drop_rate: probability of the element \n",
    "            to be multiplied by 0\n",
    "            scalar\n",
    "    # Output\n",
    "        binary mask \n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_forward(x_input, mask, drop_rate, training_phase):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguements\n",
    "        x_input: input of the layer \n",
    "            np.array of size `(n_objects, n_in)`\n",
    "        mask: binary mask\n",
    "            np.array of size `(n_objects, n_in)`\n",
    "        drop_rate: probability of the element to be multiplied by 0\n",
    "            scalar\n",
    "        training_phase: bool eiser `True` - training, or `False` - testing\n",
    "    # Output\n",
    "        the output of the dropout layer \n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_grad_input(x_input, grad_output, mask):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to the input of the layer\n",
    "    # Arguements\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dropout layer \n",
    "            np.array of size `(n_objects, n_in)`\n",
    "        mask: binary mask\n",
    "            np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the partial derivative of the loss with \n",
    "        respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-6af80e6e54f9>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-6af80e6e54f9>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    grad_input = ## COMPUTE THE GRADIENT OF THE OUTPUT WITH RESPECT TO THE INPUT ##\u001b[0m\n\u001b[1;37m                                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Dropout(Layer):\n",
    "    \n",
    "    def __init__(self, drop_rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mask = 1.0\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        if self.training_phase:\n",
    "            self.mask = dropout_generate_mask(x_input.shape, self.drop_rate)\n",
    "        self.output = dropout_\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        grad_input = ## COMPUTE THE GRADIENT OF THE OUTPUT WITH RESPECT TO THE INPUT ##\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x_input):\n",
    "    \"\"\"relu nonlinearity\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output\n",
    "\n",
    "def relu_grad_input(x_input, grad_output):\n",
    "    \"\"\"relu nonlinearity gradient\n",
    "    # Arguements\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "            grad_output: np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = relu_forward(x_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return relu_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        normalized = x_input - x_input.max(1, keepdims=True)\n",
    "        exp_ = np.exp(normalized)\n",
    "        self.output = exp_ / exp_.sum(1).reshape(-1, 1)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        y_dy = grad_output * self.output\n",
    "        sum_y_dy = np.sum(y_dy, axis=1, keepdims=True)\n",
    "        return y_dy - sum_y_dy * self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequentialNN(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        for layer in self.layers:\n",
    "            self.output = layer.forward(self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        inputs = [x_input] + [l.output for l in self.layers[:-1]]\n",
    "        \n",
    "        for input_, layer_ in zip(inputs[::-1], self.layers[::-1]):\n",
    "            grad_output = layer_.backward(input_, grad_output)\n",
    "            \n",
    "    \n",
    "    def get_params(self):\n",
    "        return [layer.get_params() for layer in self.layers]\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        return [layer.get_params_gradients() for layer in self.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.output = 0.0\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_forward(y_pred, y_true):\n",
    "    \"\"\"Compute the value of Hinge loss \n",
    "        for a given prediction and the ground truth\n",
    "    # Arguements\n",
    "        y_pred: predictions - np.array of size `(n_objects,)`\n",
    "        y_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the value of Hinge loss \n",
    "        for a given prediction and the ground truth\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_grad_input(y_pred, y_true):\n",
    "    \"\"\"Compute the partial derivative \n",
    "        of Hinge loss with respect to its input\n",
    "    # Arguements\n",
    "        y_pred: predictions - np.array of size `(n_objects,)`\n",
    "        y_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the partial derivative \n",
    "        of Hinge loss with respect to its input\n",
    "        np.array of size `(n_objects,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hinge(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.output = hinge_forward(y_pred, y_true)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        return hinge_grad_input(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2_regularizer(weight_decay, weights):\n",
    "    \"\"\"Compute the L2 regularization term\n",
    "    # Arguements\n",
    "        weight_decay: float\n",
    "        weights: list of arrays of different shapes\n",
    "    # Output\n",
    "        sum of the L2 norms of the input weights\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''This is a basic class. \n",
    "    All other optimizers will inherit it\n",
    "    '''\n",
    "    def __init__(self, model, lr=0.01, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def update_params(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    '''Stochastic gradient descent optimizer\n",
    "    https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "    '''\n",
    "        \n",
    "    def update_params(self):\n",
    "        weights = self.model.get_params()\n",
    "        grads = self.model.get_params_gradients()\n",
    "        for w, dw in zip(weights, grads):\n",
    "            update = self.lr * dw + self.weight_decay * w\n",
    "            # it writes the result to the previous variable instead of copying\n",
    "            np.subtract(w, update, out=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the following lines of code we will give the input (images) and the targets (labels) to the network furthermore, we also need to set the input and the output size of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-bdbcc66b0f29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHinge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_input' is not defined"
     ]
    }
   ],
   "source": [
    "### YOUR CODE###\n",
    "#Specify the input size for the network\n",
    "#specify the output size for the network\n",
    "#specify the inputs for the network\n",
    "#specify the outputs for the network\n",
    "#num_input=\n",
    "#num_output=\n",
    "#X=\n",
    "#y=\n",
    "###\n",
    "model = SequentialNN()\n",
    "model.add(Dense(num_input,num_output))\n",
    "coeff_l2 = 0.0\n",
    "loss = Hinge()\n",
    "sgd = SGD(model, lr=0.1, weight_decay=coeff_l2 * 2.0)\n",
    "\n",
    "for i in range(200):\n",
    "    y_pred = model.forward(X)\n",
    "    loss_value = loss.forward(y_pred, Y) + coeff_l2 * l2_regularizer(model.get_params())\n",
    "    loss_grad = loss.backward(y_pred, Y)\n",
    "    model.backward(X, loss_grad)\n",
    "    sgd.update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the network should be tested on test data, images in this task. During the test time unlabeled input is given to the network and by using the trained weights from the training cycle of the network the ouput class for the unlabeled input is predicted. The figure below shows the difference between the training and the testing of the network.\n",
    "![test](./images/Test.PNG)\n",
    "In the following cell implement the code for testing the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing the network \n",
    "###YOUR CODE FOR TESTING THE NETWORK  ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
