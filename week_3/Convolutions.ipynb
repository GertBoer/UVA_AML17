{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "The main idea of this assignment is to understand the convolutional neural networks and the basics of image filtering. Matrix convolution and convolutional layer will be implemented from scratch. \n",
    "\n",
    "All functions should be implemented in **NumPy** if no other notes are given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "* [1. Recap](#1.-Recap)\n",
    "* [2. Matrix Convolution](#2.-Matrix-Convolution)\n",
    "* [3. Basic Kernels](#3.-Basic-Kernels)\n",
    "* [4. Convolutional Layer](#4.-Convolutional-Layer)\n",
    "* [5. MaxPooling Layer](#5.-MaxPooling-Layer)\n",
    "* [6. CNN](#6.-CNN)\n",
    "* [7. Experiments](#7.-Experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recap\n",
    "\n",
    "During the previous assignment, you implemented the main building blocks of the neural networks: **Dense Layer**, nonlinearities, losses, and optimizers.\n",
    "* Dense layer is useful enough\n",
    "* Dense layer performs the following mapping of the input matrix $X$ (matrix of objects): \n",
    "$$\n",
    "X \\rightarrow XW + b\n",
    "$$\n",
    "* It allows one to build and train flexible models \n",
    "* Let's look precisely at image processing with Dense Layer\n",
    "    * We have a grayscale image $x$ of size $N \\times M$\n",
    "    * We reshape it into a vector of length $NM$\n",
    "    * Then we map it with a dense layer\n",
    "    * And obtain the transformed vector $y$\n",
    "    * Each element of $y$ depends on each element of $x$. That's why it is also called **Fully-Connected**\n",
    "* When we work with images, we assume that each pixel is correlated with its neighbours and close pixels. Distant pixels are not corellated. Various experiments demonstrate that this assumption is correct.\n",
    "* Dense layer captures these corellations, but it also captures *noisy* corellations. \n",
    "* There is a way to create **Locally-Connected** layer which will learn only local corellations with less number of parameters.\n",
    "* This layer is called **Convolutional Layer** and it is based on **matrix convolution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Matrix Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easier to understand the convolution when you see the image. Here is the image. ![](https://camo.githubusercontent.com/709b7f5eb5203b41f9456f887787b6ea790878b5/68747470733a2f2f636f6d6d756e6974792e61726d2e636f6d2f6366732d66696c652f5f5f6b65792f636f6d6d756e6974797365727665722d626c6f67732d636f6d706f6e656e74732d7765626c6f6766696c65732f30302d30302d30302d32302d36362f343738362e636f6e762e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We \"put\" the kernel on the matrix. Each element from the kernel is multiplied by the correcponding element of the source matrix. The results are summed and are written to the new matrix.\n",
    "\n",
    "* The source matrix has smaller size than the source one. It is so because of the border effects. \n",
    "* In order to obtain the matrix of the same size, zero padding could be used. \n",
    "\n",
    "* We have a matrix $X$ of size $N \\times M$ and a kernel K of size $(2p+1) \\times (2q +1 )$. \n",
    "* We also define $X_{ij} = 0$ for $i > N, i < 1$ and $j > M, j < 1$. It is called **zero padding**\n",
    "\n",
    "* Therefore the convolution of matrix with the kernel is defined as follows:\n",
    "\n",
    "$$\n",
    "Y = X \\star K \\\\\n",
    "Y_{ij} = \\sum\\limits_{\\alpha=1}^{2p + 1} \\sum\\limits_{\\beta=1}^{2q + 1}\n",
    "K_{\\alpha \\beta} X_{i + \\alpha, j+\\beta}\n",
    "$$\n",
    "\n",
    "* In machine learning this operation is called **convolution** and in mathematics it is **cross-corellation**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username already registered.\n"
     ]
    }
   ],
   "source": [
    "import automark as am\n",
    "\n",
    "username = 'sosnovik'\n",
    "# if yor are not registered\n",
    "am.register_id(username, ('ivan sosnovik', 'i.sosnovik@uva.nl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should implement matrix convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_matrix(matrix, kernel):\n",
    "    \"\"\"Perform the convolution of the matrix with the kernel\n",
    "    # Arguments\n",
    "        matrix: input matrix np.array of size `(N, M)`\n",
    "        kernel: kernel of the convolution \n",
    "            np.array of size `(2p + 1, 2q + 1)`\n",
    "    # Output\n",
    "        the result of the convolution\n",
    "        np.array of size `(N, M)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "2 & 3 & 4 \\\\\n",
    "3 & 4 & 5 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix} \\quad \n",
    "X \\star K = \n",
    "\\begin{bmatrix}\n",
    "4 & 6 & 3 \\\\\n",
    "6 & 9 & 6 \\\\\n",
    "3 & 6 & 8 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [2, 3, 4],\n",
    "    [3, 4, 5]\n",
    "])\n",
    "\n",
    "K = np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  6.  3.]\n",
      " [ 6.  9.  6.]\n",
      " [ 3.  6.  8.]]\n"
     ]
    }
   ],
   "source": [
    "print(conv_matrix(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "am.test_student_function(username, conv_matrix, ['matrix', 'kernel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Basic Kernels\n",
    "\n",
    "Matrix convolution could be used to process the image: ro blur it, to shift the image, to get the edges etc. Here is the very interesting [article](http://setosa.io/ev/image-kernels/). It is interactive. So you are able to get the better understanding of convolutions. \n",
    "\n",
    "Let's play with convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_img = plt.imread('./images/dog.png')\n",
    "plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert it to grayscale. RGB image is a 3 dimensional tensor. But grayscale image could be represented as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = rgb_img.mean(axis=2)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's blur the image with [box blur](https://en.wikipedia.org/wiki/Box_blur). It is just a convolution of a matrix with the kernel of size $N \\times N$ of the following form:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N^2}\n",
    "\\begin{bmatrix}\n",
    "1 & \\dots  & 1\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "1 & \\dots  & 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def box_blur(image, box_size):\n",
    "    \"\"\"Perform the blur of the image\n",
    "    # Arguments\n",
    "        image: input matrix - np.array of size `(N, M)`\n",
    "        box_size: the size of the blur kernel - int > 0  \n",
    "            the kernel is of size `(box_size, box_size)`\n",
    "    # Output\n",
    "        the result of the blur\n",
    "            np.array of size `(N, M)`\n",
    "    \"\"\"   \n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blur_dog = box_blur(img, box_size=5)\n",
    "plt.imshow(blur_dog, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "am.test_student_function(username, box_blur, ['image', 'box_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get the vertical and horizontal gradients. To perform it we just calculate the convolution of the image with the following kernels:\n",
    "\n",
    "$$\n",
    "K_h = \n",
    "\\begin{bmatrix}\n",
    "-1 & 0  & 1\\\\\n",
    "\\end{bmatrix} \\quad\n",
    "K_v = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix} \\\\\n",
    "X_h = X \\star K_h \\quad X_v = X \\star K_v\\\\\n",
    "$$\n",
    "\n",
    "And then we just calculate the amplitude of the gradient in both directions:\n",
    "\n",
    "$$\n",
    "X_\\text{grad} = \\sqrt{X_h^2 + X_v^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dog_h = conv_matrix(blur_dog, np.array([[-1, 0, 1]]))\n",
    "dog_v = conv_matrix(blur_dog, np.array([[-1, 0, 1]]).T)\n",
    "dog_grad = np.sqrt(dog_h ** 2 + dog_v ** 2)\n",
    "plt.imshow(dog_grad, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the edges we can work with. This is not the only way to get the edges. There are plenty of them:\n",
    "* [Canny edge detection](https://en.wikipedia.org/wiki/Canny_edge_detector)\n",
    "* [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator)\n",
    "* [Prewitt operator](https://en.wikipedia.org/wiki/Prewitt_operator)\n",
    "\n",
    "We fixed the kernels and used them. But we can also learn them such by minimizing sime loss and making the processing as effective as it is possible. To do it, we have to define **Convolutional layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
